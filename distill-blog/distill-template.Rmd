---
title: "Student Loan Default Predictor"
description: |
 In this work, We present results for different machine learning methods to predict student loan default in FIES, the most prominent student loan program in Brazil.
author: 
  - name: Bruno Ponne
  - name: Carol Sobral
  - name: Diego Faria
date: "`r Sys.Date()`" 
categories: 
  - Machine Learning
  - Credit Scoring
  - Load Default Predictions
  - Ensemble Models
creative_commons: CC BY
repository_url: https://github.com/cbsobral/ml-fies
output: 
  distill::distill_article: 
    self_contained: false
preview: figures/BERTfig3.png
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Load dependencies 
library(reticulate) # For rendering Python code 
```

## Abstract 


[@kim_convolutional_2014]
This paper presents results for different machine learning methods to predict student loan default in FIES, the most prominent student loan program in Brazil. We employed  five methods: logistic regression, decision trees, support vector machines, random forest, and neural networks. The neural networks showed the best results, followed by random forest, logistic regression, linear SVM, and decision trees. 

The best models were then combined in an ensemble model using Weighted Average (AvgW). Compared to the baseline, the results were improved by 6.6%. The values of the area under the curve obtained exceeded those available in the related literature. 

Based on a feature importance analysis, loan-value related features are the most important predictors of loan defaults.


## Introduction

Machine learning algorithms have been used extensively to predict credit risk in the context of consumer loans (Kruppa et al., 2013; Yeh and hui Lien, 2009). Educational loans are similar to consumer loans in many respects but have some unique features, such as increased uncertainty
about the borrower’s characteristics at the time of repayment. In addition, student loans are tied to more extended repayment periods and little or no collateral, leading to more difficult lender risk assessment.

In Brazil, the most prominent educational loan program is FIES (Financiamento Estudantil). Its goal is to provide loans to students with better interest rates compared to the market; the program is partially public-funded. Between 2009 and 2015, more than 2 million students received a loan
through the FIES program. We used a database that contains information on more than 600,000 students who received the loan from the FIES program. The database refers to the position in 2021 of loans contracted in 2015. Each row of the dataset corresponds to a contract and contains information on loan compliance, the number of days of noncompliance, and a loan default dummy. Also, for each contract, there is information about each student, such as family income, whether or not she has a job, gender, race, what course she is taking, marital status, whether or not the student attended a public high school, and so on. Each of these variables is a possible predictor of loan default. The data sources are the Ministry of Education and the financial institutions that administer the loan; the Brazilian National Treasury compiled the final database.


This paper adds to the existing literature by analyzing novel data on the FIES program released by the National Treasury Secretariat to investigate the classification accuracy of machine learning algorithms for screening education borrowers. In addition, we explore the key drivers of repayment to provide key insights into this particular setting and provide evidence-based policy recommendations to promote the sustainability of this loan fund. 


Finally, we provide insights into the ethical implications of our findings
for the student loan granting process in Brazil. As expected, our results show that an ensemble model is better able to provide accurate classification information about student credit risk for such a dataset, as our ensemble model outperforms all non-ensemble models. Loan value related features are the most important predictors of loan default in our model. This could be a source of bias if we consider that subgroups borrowing certain loan values could be included or excluded when applying our model.



## Related Work 

This work relates to studies that apply machine learning algorithms to credit scoring applications. Han (2019) employs many different algorithms to predict loan repayment for a lending club, focusing mainly on the insights gained from the predictive features. The algorithm with the best performance was logistic regression, followed by random forest, k-nearest neighbors, and support vector machines.

Liang et al. (2019) use five different algorithms to accomplish the same task.The authors find that logistic regression, multilayer perceptron (neural networks), and random forest are the best algorithms. In addition, number of days employed and age are the strongest predictors of loan repayment in their data.

Lessmann et al. (2015) conduct a benchmark study for 41 classification methods over eight credit scoring datasets. The results show that many classifiers predict credit default outcomes better than the industry standard – logistic regression. Heterogeneous ensemble classifiers, such as direct selective ensembles, perform remarkably well in their study.

Stelzer (2019) conducts a similar benchmark study using 23 machine learning methods. She applies five data sampling strategies to combat existing class imbalances in the data. The results show that simple up-sampling or down-sampling yields the best predictive performance in the face of unequal class distributions. 

In the Brazilian context, Aniceto et al. (2020) study the performance of different methods for predicting loan defaults with a dataset of loans provided by a Brazilian bank. They use logistic regression as a baseline and conclude that ensemble methods perform better.

In the context of student loans, in a working paper, Jayadev et al. (2019) apply machine learning models to predict defaults on education loans, taking into account borrower-specific characteristics such as income, university, geographic region, as well as systematic factors such as growth rate, inflation, and unemployment rate. They find that ”ensemble models tend to perform better than simple artificial techniques and statistical models and that the performance can be improved significantly by model stacking” (p. 27).


**See below for an example of how to cite related work in Markdown.**

Bidirectional Encoder Representations from Transformers (BERT) have proven successful in prior attempts to classify phrases and short texts [@devlin2018bert].

**Footnotes and Sidenotes**

You can use footnotes ^[This is a footnote. You can view this by hovering over the footnote in text.] or sidenotes to elaborate on a concept throughout the paper. 

<aside>
This is a side note. 
</aside>

## Proposed Method 

### Logistic Regression

Logistic Regression (LR) is a binary classifier that estimates the probability of a certain instance belonging to one of two classes. According to o Yeh and hui Lien (2009), a logistic regression model specifies that the probability of an event is a linear function of the observed values of the available predictors. 

### Decision Trees

Decision Trees apply a decision-tree-like structure in order to partition the data and separate cases of the same class in the same subgroups. For that end, they produce a rooted tree consisting of nodes split upon each variable, resulting in a final tree where branches represent an outcome of the splits. Each final node represents a class that is assigned to the data Stelzer (2019). There are distinct types of classification and decision trees, and for this study, we apply the Classification and Regression Trees (CART) technique. 

### Random Forest

Random Forests are a type of ensemble classifier that combines tree models and the principles of both bagging and boosting Stelzer (2019). They make use of Decision Trees trained on different random subsets of the dataset to introduce further diversity to estimates. After a set number of trees has been trained, the prediction is produced by collecting all the trees’ votes.

### Support Vector Machines

This technique is based on the idea that the space where the labels are located can be divided in two parts. In order to classify each label, a hyperplane is employed to set the limit between two classes. In our case, an hyperplane would separate the space to distinguish a good from a bad borrower. As pointed out by Bishop (2016), there are multiple hyperplanes that might accomplish that task, but we would like to find the one which minimizes the generalization error. The Support Vector Machines (SVM) approach uses the concept of margin to find the best solution. Margin is the distance between the decision boundary and the closest vector locating an instance from our training set. These vectors are known as support vectors. The technique aims at maximizing this margin while avoiding or limiting margin violations Geron (2019). Mathematically, it is possible to express this classification as follows:

 $$ y(x) = w^T \Phi (t) $$

Where $w^T$ is a vector of coefficients that maximizes the margin around the classification limit and minimizes errors.  $\Phi (x)$ comprises the values of each feature that might be transformed in order to classify data that is not linear. This is important because linear SVMs can perform poorly in data that is not linearly separable in space.  Finally, `$b$` represents the bias. After training the model and finding `$w^T$`, it is possible to find the class of a new instance by observing the signal of $y$. 

### Artificial Neural Network

Artificial Neural Networks (ANN) rely on Threshold Logic Units (TLU) or other kinds of artificial neurons to carry out classification. TLU is an artificial neuron that based on a set of features inputs computes a weighted sum which is compared to a threshold. The output depends on whether the weighted sum is lower or higher than the threshold and tells the class of a particular new instance Geron (2019). As it can be seen, ANN shares some similarities with SVM but the latter uses only critical features to establish the threshold, the so called support vectors, while the former uses all the features.

### Baseline Model 

Traditionally, the main literature on credit score prediction employs logistic regression (LR) as a baseline for comparison. Aniceto (2020) study the performance of several methods to predict loan default with a data set provided by a Brazilian bank. They use logistic regression as a baseline and conclude that ensemble methods perform better. In this research, however, the recommendation of Lessman et al. (2015) will be followed. The authors suggest that random forest (RF) should be used as the benchmark when exploring the performance of new models for default prediction. They argue that RF has proved to be superior to LR and therefore outperforming LR models is no longer a major signal of methodological advancement.

### Ensemble Model using Weighted Average (AvgW)

After training and adjusting the individual models aforementioned,  our proposal was to build a classifier whose output is the weighted average of the individual methods. Bishop (2016  highlights that very often a combination of multiple models perform better than a single isolated one. In order to implement our ensemble method, we followed the approach employed by Stelzer (2019). The author uses a weighted average to estimate default probability. Each model is weighted according to its accuracy so that better performing methods have a higher influence over the final prediction.  This method is also found to be one of the top performers among 41 other approaches to predict credit scoring Lessman et al. (2015). 

The final predicted class was given by the following procedure. First, the weighted average probability is calculated for each class according to the expression below:

$$ {Weighted Average Prob}_{c} = \sum_{i=1}^{n} \frac{p_{cj}{w_i}}{n} $$

For each instance, the model computes the weighted average probability p for class c by multiplying the probability of class c given by model i by the weight of model i and dividing the result by the number of models n.  The predicted class is the one with the highest weighted average probability. The weights of each model were defined proportional to the AUC (area under the curve) obtained for the respective model.

## Experiments 

**Data**: Describe the dataset(s) you are using (provide references). If it's not already clear, make sure the associated task is clearly described.
 
**Software**: Briefly list (and cite) software you used.

**Hardware**: If relevant, list hardware resources you used.

**Evaluation method**: Describe the evaluation metric(s) you used, plus any other details necessary to understand your evaluation.

**Experimental details**: How you ran your experiments (e.g. model configurations, learning rate, training time, etc.) 

**Results**: Report the quantitative results that you have found so far. Use a table or plot to compare multiple results and compare against baselines. 

**Comment on quantitative results**: Are they what you expected? Better than you expected? Worse than you expected? Why do you think that is? What does this tell you about what you should do next? Including training curves might be useful to discuss whether things are training effectively.

***Note***: **Feel free to use some of the code from your project to explain your experiments. See example code block below.**

```{python bertcnn model parameters, echo = TRUE, eval = FALSE}
#OUTPUT_DIM = len(LABEL.vocab#)
#DROPOUT = 0.5
#N_FILTERS = 100
#FILTER_SIZES = [2,3]

#model = BERTCNN(bert,
                OUTPUT_DIM,
                DROPOUT,
                N_FILTERS,
                FILTER_SIZES)
```

## Analysis 


The results after the optimization routines can be summarized in the following table along with two important references. The Logistic Regression results and the Random Forest results. The first one, largely used in the literature as baseline for this type of classification and the second one, used in this work as a good reference, due to it's expected good results.

```{r,echo = FALSE}
table_analysis <- data.frame(Methods=c('Random Forest','Logistic Regression'), AUC_Baseline =c(0.7836,0.7232),AUC_AvgW = c(0.8354,0.8354),Improvement = c("6.6%","15.5%"))
```

```{r, echo = FALSE}
library(dplyr)
library(kableExtra)

table_analysis %>%kbl() %>%  kable_styling()

```
The improvements achieved were remarkable. Although, we probably could get even more improvements if we had used more models in the AvgW routine.

The final importance of that feature is estimated by calculating how much the score of the model decreases when this feature is substituted by noise.Total debt is the most important predictor in our ensemble model. It contains information on the total value of the debt of a certain student. It is related to which course the student chose, since some courses, like Medicine, are more expensive. It is also related to other features that also appear among the most important, like semesters funded, loan value, loan limit and loan value per semester. Our model is, therefore, strongly driven by the amount of money hired by the student.


The next figure shows the most important features of our model. The most important features for the prections are listed in the figure XXX . The 10 most important features are mainly related to the loan conditions.

```{r fig2, eval = TRUE, echo = FALSE, out.width = '100%', fig.cap = "Feature Importance"}
knitr::include_graphics("./figures/FeatureImportance.png")
```

### Ethical Implications

The use of Machine Learning to predict loan default is one among the many analyses that are now largely employing ML algorithms like credit scoring and crime prediction. Even though,  our analysis is mainly focused on a proposal for a better public policy, we cannot avoid the ethical discussion about the results, considering that according to  \cite{Kusner2017} “decisions in these areas may have ethical or legal implications, so it is necessary for the modeler to think beyond the objective of maximizing prediction accuracy and consider the societal impact of their worl."

That being said, we should highlight that the most important features of our ensemble model are related to the value of the course chosen by the student or to his capacity to pay, as poorer students, for example, would finance a higher proportion of their courses. Because of that, applying our model to define who should get a loan without further thought might exclude subgroups based on loan value. 

###  Robnustness Withouth Fixed Characteristics

In terms of fixed characteristics in the model, that can lead to misinterpretation of the results, due to a possible misinterpretation of the results,  we analyse “gender” and “ethnicity”. For our proposal it is important to keep these features in the model, but we also tested the model  without these characteristics. The  results are also robust without these features as shown in the following table, and still outperform  our baseline model as well as the findings from previous literature.

```{r}
remove_characteristics <- data.frame(Methods=c('Without Gender','Without Ethnicity','Without Gender and Ethnicity'), AUC =c(0.753,0.751,0.752), Brier_Score = c(0.201,0.202,0.202))
```


```{r}

remove_characteristics %>%kbl() %>%  kable_styling()

```


## Conclusion(s)

Summarize the main findings of your project, and what you learned. Highlight your achievements, and note the primary limitations of your work. If you like, you can describe avenues for future work.

## Acknowledgments 

List acknowledgments, if any. For example, if someone provided you a dataset, or you used someone else's resources, this is a good place to acknowledge the help or support you received.