---
title: "Student Loan Default Predictor"
description: |
 In this work, We present results for different machine learning methods to predict student loan default in FIES, the most prominent student loan program in Brazil.
author: 
  - name: Bruno Ponne
  - name: Carol Sobral
  - name: Diego Faria
date: "`r Sys.Date()`" 
categories: 
  - Machine Learning
  - Credit Scoring
  - Load Default Predictions
  - Ensemble Models
creative_commons: CC BY
repository_url: https://github.com/cbsobral/ml-fies
output: 
  distill::distill_article: 
    self_contained: false
preview: figures/BERTfig3.png
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Load dependencies 
library(reticulate) # For rendering Python code 
```

## Abstract 


This work presents the results for different machine learning methods for predicting student loan default in FIES, the best-known student loan program in Brazil. We used five methods: logistic regression, decision trees, support vector machines, random forest, and neural networks. The neural network classifier outperformed the other methods. After fine-tuning the three best models, they were combined into an ensemble using the Weighted Average (AvgW) method. Compared to the baseline, the predictions were improved by 6.6%. The values of the area under the curve obtained exceeded those found in the related literature. Loan value-related features were the most important predictors of loan defaults, based on a feature importance analysis.


## Introduction

Machine learning algorithms have been widely used to predict credit risk in the context of consumer loans [@Kruppa2013; @Yeh2009]. Education loans are similar to consumer loans in many ways but have some unique characteristics, such as increased uncertainty about the borrower's characteristics at the time of repayment. In addition, student loans are tied to longer repayment periods and little or no collateral, leading to a more complex risk assessment for lenders.

In Brazil, the best-known education loan program is FIES (Financiamento Estudantil). Its goal is to provide loans to students at more favorable interest rates than the market; the program is partially publicly funded. Between 2009 and 2015, more than 2 million students received loans through the FIES program. We used a database that contains information on more than 600,000 students who received a loan from the FIES program. The database refers to the position in 2021 of loans contracted in 2015. Each row in the dataset corresponds to a contract and contains information on compliance with loan terms, the number of days of noncompliance, and a dummy for loan default. Also, for each contract, there is information about each student, such as family income, whether or not she has a job, gender, race, what course she is taking, marital status, whether or not the student attended a public high school, and so on. Each of these variables is a possible predictor of loan default. The data sources are the Ministry of Education and the financial institutions that administer the loan; the Brazilian National Treasury compiled the final database.

This work adds to the existing literature by analyzing new data on the FIES program released by the National Treasury Secretariat to investigate the classification accuracy of machine learning algorithms for screening education borrowers. We also examine the key drivers of repayment to provide essential insights into this particular setting and provide evidence-based policy recommendations to promote the sustainability of this fund. 

Finally, we provide insights into the ethical implications of our findings for the student loan granting process in Brazil. As expected, our results show that an ensemble model is better able to provide accurate classification information about student credit risk for such a dataset, as our ensemble model outperforms all non-ensemble models. Loan value-related features are the most important predictors of loan default in our model. This could be a source of bias if we consider that subgroups that borrow specific loan values could be included or excluded when applying our model.




## Related Work 

Our work relates to studies that apply machine learning algorithms to credit scoring applications. @Han_2019 employs many different algorithms to predict loan repayment for a lending club, focusing mainly on the insights gained from the predictive features. The algorithm with the best performance was logistic regression, followed by random forest, k-nearest neighbors, and support vector machines.


@Lessman-2015 conduct a benchmark study for 41 classification methods over eight credit scoring datasets. The results show that many classifiers predict credit default outcomes better than the industry standard – logistic regression. Heterogeneous ensemble classifiers, such as direct selective ensembles, perform remarkably well in their study. @Stelzer2019 conducts a similar benchmark study using 23 machine learning methods. She applies five data sampling strategies to combat existing class imbalances in the data. The results show that simple up-sampling or down-sampling yields the best predictive performance in the face of unequal class distributions. 

In the Brazilian context, @Aniceto2020 study the performance of different methods for predicting loan defaults with a dataset of loans provided by a Brazilian bank. They use logistic regression as a baseline and conclude that ensemble methods perform better.

In the context of student loans, in a working paper, @Jayadev2019 apply machine learning models to predict defaults on education loans, taking into account borrower-specific characteristics such as income, university, geographic region, as well as systematic factors such as growth rate, inflation, and unemployment rate. They find that ”ensemble models tend to perform better than simple artificial techniques and statistical models and that the performance can be improved significantly by model stacking” (p. 27).


## Proposed Method 

### Logistic Regression

Logistic Regression (LR) is a binary classifier that estimates the probability of a certain instance belonging to one of two classes. According to o Yeh and Hui Lien (2009), a logistic regression model specifies that the probability of an event is a linear function of the observed values of the available predictors. 

### Decision Trees

Decision Trees apply a decision-tree-like structure in order to partition the data and separate cases of the same class in the same subgroups. For that end, they produce a rooted tree consisting of nodes split upon each variable, resulting in a final tree where branches represent an outcome of the splits. Each final node represents a class that is assigned to the data Stelzer (2019). There are distinct types of classification and decision trees, and for this study, we apply the Classification and Regression Trees (CART) technique. 

### Random Forest

Random Forests are a type of ensemble classifier that combines tree models and the principles of both bagging and boosting Stelzer (2019). They make use of Decision Trees trained on different random subsets of the dataset to introduce further diversity to estimates. After a set number of trees has been trained, the prediction is produced by collecting all the trees’ votes.

### Support Vector Machines

This technique is based on the idea that the space where the labels are located can be divided in two parts. In order to classify each label, a hyperplane is employed to set the limit between two classes. In our case, an hyperplane would separate the space to distinguish a good from a bad borrower. As pointed out by Bishop (2016), there are multiple hyperplanes that might accomplish that task, but we would like to find the one which minimizes the generalization error. The Support Vector Machines (SVM) approach uses the concept of margin to find the best solution. Margin is the distance between the decision boundary and the closest vector locating an instance from our training set. These vectors are known as support vectors. The technique aims at maximizing this margin while avoiding or limiting margin violations Geron (2019). Mathematically, it is possible to express this classification as follows:

 $$ y(x) = w^T \Phi (t) $$

Where $w^T$ is a vector of coefficients that maximizes the margin around the classification limit and minimizes errors.  $\Phi (x)$ comprises the values of each feature that might be transformed in order to classify data that is not linear. This is important because linear SVMs can perform poorly in data that is not linearly separable in space.  Finally, `$b$` represents the bias. After training the model and finding `$w^T$`, it is possible to find the class of a new instance by observing the signal of $y$. 

### Artificial Neural Network

Artificial Neural Networks (ANN) rely on Threshold Logic Units (TLU) or other kinds of artificial neurons to carry out classification. TLU is an artificial neuron that based on a set of features inputs computes a weighted sum which is compared to a threshold. The output depends on whether the weighted sum is lower or higher than the threshold and tells the class of a particular new instance Geron (2019). As it can be seen, ANN shares some similarities with SVM but the latter uses only critical features to establish the threshold, the so called support vectors, while the former uses all the features.

### Baseline Model 

Traditionally, the main literature on credit score prediction employs logistic regression (LR) as a baseline for comparison. Aniceto (2020) study the performance of several methods to predict loan default with a data set provided by a Brazilian bank. They use logistic regression as a baseline and conclude that ensemble methods perform better. In this research, however, the recommendation of Lessman et al. (2015) will be followed. The authors suggest that random forest (RF) should be used as the benchmark when exploring the performance of new models for default prediction. They argue that RF has proved to be superior to LR and therefore outperforming LR models is no longer a major signal of methodological advancement.

### Ensemble Model using Weighted Average (AvgW)

After training and adjusting the individual models aforementioned,  our proposal was to build a classifier whose output is the weighted average of the individual methods. Bishop (2016  highlights that very often a combination of multiple models perform better than a single isolated one. In order to implement our ensemble method, we followed the approach employed by Stelzer (2019). The author uses a weighted average to estimate default probability. Each model is weighted according to its accuracy so that better performing methods have a higher influence over the final prediction.  This method is also found to be one of the top performers among 41 other approaches to predict credit scoring Lessman et al. (2015). 

The final predicted class was given by the following procedure. First, the weighted average probability is calculated for each class according to the expression below:

$$ {Weighted Average Prob}_{c} = \sum_{i=1}^{n} \frac{p_{cj}{w_i}}{n} $$

For each instance, the model computes the weighted average probability p for class c by multiplying the probability of class c given by model i by the weight of model i and dividing the result by the number of models n.  The predicted class is the one with the highest weighted average probability. The weights of each model were defined proportional to the AUC (area under the curve) obtained for the respective model.

## Experiments 

**Data**: The data set used for this project was produced by the Brazilian Ministry of Education. Each observation in the data corresponds to individual student loan operations and includes many features that can be used to predict debt loans, such as family income, the field of studies, and occupation, among others. The borrowers’ privacy is guaranteed by removing all the information about the loan taker’s identity from the data set before it was made available for this study.
 
**Software**: In order to program and organize our code, we used Jupyter notebooks, a document format for coding, displaying results and text in a form that is both readable and executable [@soton403913]. To run Jupyter notebooks we used two environments: JupyterLab and Google Colaboratory (Colab). Finally, we used Scikit-Learn to implement our models. It provides numerous algorithms to perform machine learning tasks [@scikit-learn].


**Evaluation method**: Our main performance indicator is the Receiver Operating Characteristics (ROC curve), also known as area under the curve (AUC). The AUC evaluates the discriminatory ability of a classifier. It estimates the probability that a randomly chosen positive instance of class 1 is correctly ranked higher than a randomly selected instance of negative class 0 [@article]. The AUC ranges from 0.5 to 1, where an AUC of 0.5 indicates that a classifier has no predictive value and a higher score implies better predictive performance.

**Experimental details**: Our experiments occurred in five main parts: data pre-processing, pipelines, initial training, fine tuning and final model training (ensemble model). 

* Pre-processing: The first step in pre-processing was to remove information from borrowers that were not in the repayment stage, which resulted in the exclusion of almost 200,000 observations that were not suitable for prediction purposes.Next, we removed variables that were not as relevant to the results or had a high proportion of missing values. Additionally, in order to reduce complexity and the number of levels, some categorical variables were re-coded. We also analyzed possible collinearity between variables and eliminated redundant variables to improve the efficiency of the learning process. In the following step the data was split in train and test set using the proportion of 80% for training and 20% for the testing set.

* Pipelines: Pipelines are sequences of procedures carried out to prepare the training and testing set according to requirements of the functions and classes used to train the models. For example, we used pipelines for filling missing values and for variable scaling.

* Initial Training: After preparing the data, we used Scikit-learn to train our initial models. Results are presented in the table bellow. Artificial Neural Networks was our best performer.


```{r,echo = FALSE}
library(dplyr)
library(kableExtra)


table1 <- data.frame(Method=c('Neural Network', 'Random Forest','Logistic Regression', 'Decision Trees'), AUC =c(0.8187, 0.7825, 0.7232, 0.6451))

table1 %>% kbl(caption = "Initial Models") %>%  kable_styling()
```



* Fine tuning: Employing the Scikit-learn Random Grid Search, we investigated the best hyperparameters for our three highest-scoring models (Neural Network, Random Forest, and Logistic Regression). After optimization, the neural network classifier trained on the down-sampled data set had the best performance, achieving an AUC of over 83%. 

* Ensemble model: Finally, we implemented a composite predictor, made out of the average of different types of base models. This approach is likely to improve performance because the distinct models approach data differently and therefore complement each other [@Stelzer2019]. In order to obtain an averaged model with different types of base predictors, we selected the three best-performing individual models: ANN, RF and LR. We used the Scikit-Learn VotingClassifier class to implement the ensemble method. This class offers the possibility of computing the predicted class using “soft voting” with the possibility to assign one weight to each model. As explained in the methodology section, our aim was to employ weights proportional to the AUC of each model. In practice, however, we realized that weights proportional to the AUC did not deliver the best AUC. Figure 1 shows that the performance of the model increased as we increased the weight of our best predictor. The performance reached a peak when the ANN model had a weight of around 0.8 and the remaining models shared 0.2 of the weight. When the ANN weight was further increased, the performance started to decrease again, indicating that we had found the best weight distribution for our ensemble model.

<center>

```{r fig1, eval = TRUE, echo = FALSE, out.width = '80%', fig.cap = "Optimal Weights"}
knitr::include_graphics("./figures/OptimalWeights.png")
```

</center>


## Analysis 

The results after the optimization routines can be summarized in the following table along with two important references. The Logistic Regression results and the Random Forest results. The first one, largely used in the literature as baseline for this type of classification and the second one, used in this work as a good reference, due to it's expected good results.

```{r, echo = FALSE}
table2 <- data.frame(Methods=c('Random Forest','Logistic Regression'), AUC_Baseline =c(0.7836,0.7232), AUC_AvgW = c(0.8354,0.8354),Improvement = c("6.6%","15.5%"))

table2 %>% kbl(caption = "Fine-Tuning Results") %>%  kable_styling()
```

The improvements achieved were remarkable. Although, we probably could get even more improvements if we had used more models in the AvgW routine.

The final importance of that feature is estimated by calculating how much the score of the model decreases when this feature is substituted by noise.Total debt is the most important predictor in our ensemble model. It contains information on the total value of the debt of a certain student. It is related to which course the student chose, since some courses, like Medicine, are more expensive. It is also related to other features that also appear among the most important, like semesters funded, loan value, loan limit and loan value per semester. Our model is, therefore, strongly driven by the amount of money hired by the student.


The next figure shows the most important features of our model. The most important features for the predictions are listed in the figure XXX . The 10 most important features are mainly related to the loan conditions.

<center>
```{r fig2, eval = TRUE, echo = FALSE, out.width = '100%', fig.cap = "Feature Importance"}
knitr::include_graphics("./figures/FeatureImportance.png")
```
</center>


### Ethical Implications

The use of Machine Learning to predict loan default is one among the many analyses that are now largely employing ML algorithms like credit scoring and crime prediction. Even though,  our analysis is mainly focused on a proposal for a better public policy, we cannot avoid the ethical discussion about the results, considering that according to  \cite{Kusner2017} “decisions in these areas may have ethical or legal implications, so it is necessary for the modeler to think beyond the objective of maximizing prediction accuracy and consider the societal impact of their work."

That being said, we should highlight that the most important features of our ensemble model are related to the value of the course chosen by the student or to his capacity to pay, as poorer students, for example, would finance a higher proportion of their courses. Because of that, applying our model to define who should get a loan without further thought might exclude subgroups based on loan value. 

###  Robnustness Without Fixed Characteristics

In terms of fixed characteristics in the model, that can lead to misinterpretation of the results, due to a possible misinterpretation of the results,  we analyze “gender” and “ethnicity”. For our proposal it is important to keep these features in the model, but we also tested the model  without these characteristics. The  results are also robust without these features as shown in the following table, and still outperform  our baseline model as well as the findings from previous literature.

```{r}
table3 <- data.frame(Methods=c('Without Gender','Without Ethnicity','Without Gender and Ethnicity'), AUC =c(0.753,0.751,0.752), Brier_Score = c(0.201,0.202,0.202))

table3 %>% kbl(caption = 'Robustness - Fixed Characteristics') %>%  kable_styling()
```


## Conclusion

In this study, we apply different machine learning classifiers and sampling strategies to predict student loan default in FIES. Our findings indicate that using an ensemble method is superior to most individual models. The overall results showed that Artificial Neural Networks performed best in every context. Moreover, up-sampling and down-sampling strategies were not able to remarkably improve the performance of the models in terms of AUC score. Finally, concerning feature importance, we found that loan-related characteristics - rather than individual characteristics - are the most important predictors of default.

The results obtained in this work were limited due to the high computational time required to train the selected models. Moreover, the results could be further improved by a deeper and more comprehensive analysis of the predictor variables used for each estimation.

In summary, this research clearly demonstrates that machine learning algorithms can play an important role in improving the loan granting process in an educational setting, while having the sustainability of the fund in mind. However, our analysis also raises the question of whether historical biases are perpetuated with the unrestricted use of prediction algorithms. For this reason, it is recommended that practitioners refrain from adopting machine learning techniques without regard to existing biases and policy goals.
