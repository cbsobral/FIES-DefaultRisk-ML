%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Jankin at 2019-06-23 08:30:59 +0100 


%% Saved with string encoding Unicode (UTF-8) 

@article{kim_convolutional_2014,
	Abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
	Author = {Kim, Yoon},
	Date-Added = {2019-06-23 08:30:21 +0100},
	Date-Modified = {2019-06-23 08:30:21 +0100},
	File = {arXiv\:1408.5882 PDF:C\:\\Users\\kakia\\Zotero\\storage\\VCCHETZF\\Kim - 2014 - Convolutional Neural Networks for Sentence Classif.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\kakia\\Zotero\\storage\\Z5ADPS7M\\1408.html:text/html},
	Journal = {arXiv:1408.5882 [cs]},
	Keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	Month = aug,
	Note = {arXiv: 1408.5882},
	Title = {Convolutional {Neural} {Networks} for {Sentence} {Classification}},
	Url = {http://arxiv.org/abs/1408.5882},
	Urldate = {2018-10-25},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1408.5882}}
	
@article{Suresh2019,
   abstract = {As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of potential sources of unwanted consequences. For instance, downstream harms to particular groups are often blamed on "biased data," but this concept encompass too many issues to be useful in developing solutions. In this paper, we provide a framework that partitions sources of downstream harm in machine learning into six distinct categories spanning the data generation and machine learning pipeline. We describe how these issues arise, how they are relevant to particular applications, and how they motivate different solutions. In doing so, we aim to facilitate the development of solutions that stem from an understanding of application-specific populations and data generation processes, rather than relying on general statements about what may or may not be "fair."},
   author = {Harini Suresh and John V. Guttag},
   month = {1},
   title = {A Framework for Understanding Unintended Consequences of Machine Learning},
   url = {http://arxiv.org/abs/1901.10002},
   year = {2019},
}


@article{Aniceto2020,
abstract = {Credit risk evaluation has a relevant role to financial institutions, since lending may result in real and immediate losses. In particular, default prediction is one of the most challenging activities for managing credit risk. This study analyzes the adequacy of borrower's classification models using a Brazilian bank's loan database, and exploring machine learning techniques. We develop Support Vector Machine, Decision Trees, Bagging, AdaBoost and Random Forest models, and compare their predictive accuracy with a benchmark based on a Logistic Regression model. Comparisons are analyzed based on usual classification performance metrics. Our results show that Random Forest and Adaboost perform better when compared to other models. Moreover, Support Vector Machine models show poor performance using both linear and nonlinear kernels. Our findings suggest that there are value creating opportunities for banks to improve default prediction models by exploring machine learning techniques.},
author = {Aniceto, Maisa Cardoso and Barboza, Flavio and Kimura, Herbert},
doi = {10.1186/s43093-020-00041-w},
file = {:C$\backslash$:/Users/diego/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aniceto, Barboza, Kimura - 2020 - Machine learning predictivity applied to consumer creditworthiness.pdf:pdf},
issn = {2314-7210},
journal = {Future Business Journal},
month = {dec},
number = {1},
publisher = {Springer Science and Business Media LLC},
title = {{Machine learning predictivity applied to consumer creditworthiness}},
volume = {6},
year = {2020}
}

@article{Crone2012,
abstract = {To date, best practice in sampling credit applicants has been established based largely on expert opinion, which generally recommends that small samples of 1500 instances each of both goods and bads are sufficient, and that the heavily biased datasets observed should be balanced by undersampling the majority class. Consequently, the topics of sample sizes and sample balance have not been subject to either formal study in credit scoring, or empirical evaluations across different data conditions and algorithms of varying efficiency. This paper describes an empirical study of instance sampling in predicting consumer repayment behaviour, evaluating the relative accuracies of logistic regression, discriminant analysis, decision trees and neural networks on two datasets across 20 samples of increasing size and 29 rebalanced sample distributions created by gradually under- and over-sampling the goods and bads respectively. The paper makes a practical contribution to model building on credit scoring datasets, and provides evidence that using samples larger than those recommended in credit scoring practice provides a significant increase in accuracy across algorithms. {\textcopyright} 2011.},
annote = {- 1500 instances of goods and bads would be enoughfor robust predictions
- Popular methods: Logistsic/Discrimant Analysis/RegressionTrees
- Under and oversampling techniques. Maybe important when considering effieincy
-Variables like income, better if catagorized by income.3 to 6 dummies would be sufficient [future work]
-CART cdon't perform so good with categories},
author = {Crone, Sven F. and Finlay, Steven},
doi = {10.1016/j.ijforecast.2011.07.006},
file = {:C$\backslash$:/Users/diego/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crone, Finlay - 2012 - Instance sampling in credit scoring An empirical study of sample size and balancing.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Balancing,Credit scoring,Data pre-processing,Over-sampling,Sample size,Under-sampling},
month = {jan},
number = {1},
pages = {224--238},
title = {{Instance sampling in credit scoring: An empirical study of sample size and balancing}},
volume = {28},
year = {2012}
}

@article{Kruppa2013,
abstract = {Consumer credit scoring is often considered a classification task where clients receive either a good or a bad credit status. Default probabilities provide more detailed information about the creditworthiness of consumers, and they are usually estimated by logistic regression. Here, we present a general framework for estimating individual consumer credit risks by use of machine learning methods. Since a probability is an expected value, all nonparametric regression approaches which are consistent for the mean are consistent for the probability estimation problem. Among others, random forests (RF), k-nearest neighbors (kNN), and bagged k-nearest neighbors (bNN) belong to this class of consistent nonparametric regression approaches. We apply the machine learning methods and an optimized logistic regression to a large dataset of complete payment histories of short-termed installment credits. We demonstrate probability estimation in Random Jungle, an RF package written in C++ with a generalized framework for fast tree growing, probability estimation, and classification. We also describe an algorithm for tuning the terminal node size for probability estimation. We demonstrate that regression RF outperforms the optimized logistic regression model, kNN, and bNN on the test data of the short-term installment credits. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Kruppa, Jochen and Schwarz, Alexandra and Arminger, Gerhard and Ziegler, Andreas},
doi = {10.1016/j.eswa.2013.03.019},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Credit scoring,Logistic regression,Machine learning,Probability estimation,Probability machines,Random forest},
title = {{Consumer credit risk: Individual probability estimates using machine learning}},
year = {2013}
}

@article{Stelzer2019,
abstract = {This study conducts a benchmarking study, comparing 23 different statistical and machine learning methods in a credit scoring application. In order to do so, the models' performance is evaluated over four different data sets in combination with five data sampling strategies to tackle existing class imbalances in the data. Six different performance measures are used to cover different aspects of predictive performance. The results indicate a strong superiority of ensemble methods and show that simple sampling strategies deliver better results than more sophisticated ones.},
archivePrefix = {arXiv},
arxivId = {1907.12996},
author = {Stelzer, Anna},
eprint = {1907.12996},
file = {:C$\backslash$:/Users/diego/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stelzer - 2019 - Predicting credit default probabilities using machine learning techniques in the face of unequal class distributions.pdf:pdf},
month = {jul},
title = {{Predicting credit default probabilities using machine learning techniques in the face of unequal class distributions}},
url = {http://arxiv.org/abs/1907.12996},
year = {2019}
}

@article{article,
author = {Baesens, Bart and Van Gestel, Tony and Viaene, Stijn and STEPANOVA, M. and Suykens, Johan and Vanthienen, Jan},
year = {2003},
month = {06},
pages = {},
title = {Benchmarking state-of-the-art classification algorithms for credit scoring},
volume = {54},
journal = {Journal of the Operational Research Society},
doi = {10.1057/palgrave.jors.2601545}
}

@article{Jayadev2019,
author = {Jayadev, M and Shah, Neel and Vadlamani, Ravi},
doi = {10.2139/ssrn.3510012},
file = {:C\:/Users/carol/Desktop/Spring 2021/ML/Project/Bibliography/SSRN-id3510012.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
mendeley-groups = {ML},
number = {601},
title = {{Predicting Educational Loan Defaults: Application of Artificial Intelligence Models}},
year = {2019}
}

@book{Geron2019,
Abstract = {Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how.By using concrete examples, minimal theory, and two production-ready Python frameworks—Scikit-Learn and TensorFlow—author Aurélien Géron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started.Explore the machine learning landscape, particularly neural netsUse Scikit-Learn to track an example machine-learning project end-to-endExplore several training models, including support vector machines, decision trees, rando},
Author = {Aurélien, Géron},
ISBN = {9781492032649},
Publisher = {O'Reilly Media},
Title = {Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow : Concepts, Tools, and Techniques to Build Intelligent Systems.},
Volume = {Second edition},
Year = {2019},
}



@article{Kusner2017,
   abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
   author = {Matt Kusner and Joshua Loftus and Chris Russell and Ricardo Silva},
   title = {Counterfactual Fairness},
   url = {https://obamawhitehouse.archives.gov/blog/2016/05/04/big-risks-big-opportunities-intersection-big-data-},
   year = {2017},
}


@article{Lessman-2015,
abstract = {Many years have passed since Baesens et al. published their benchmarking study of classification algorithms in credit scoring [Baesens, B.; Van Gestel, T.; Viaene, S.; Stepanova, M.; Suykens, J.; {\&} Vanthienen, J. (2003). Benchmarking state-of-the-art classification algorithms for credit scoring. Journal of the Operational Research Society, 54(6), 627-635.]. The interest in prediction methods for scorecard development is unbroken. However, there have been several advancements including novel learning methods, performance measures and techniques to reliably compare different classifiers, which the credit scoring literature does not reflect. To close these research gaps, we update the study of Baesens et al. and compare several novel classification algorithms to the state-of-the-art in credit scoring. In addition, we examine the extent to which the assessment of alternative scorecards differs across established and novel indicators of predictive accuracy. Finally, we explore whether more accurate classifiers are managerial meaningful. Our study provides valuable insight for professionals and academics in credit scoring. It helps practitioners to stay abreast of technical advancements in predictive modeling. From an academic point of view, the study provides an independent assessment of recent scoring methods and offers a new baseline to which future approaches can be compared.},
author = {Lessmann, Stefan and Baesens, Bart and Seow, Hsin Vonn and Thomas, Lyn C.},
doi = {10.1016/j.ejor.2015.05.030},
file = {:C$\backslash$:/Users/diego/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lessmann et al. - 2015 - Benchmarking state-of-the-art classification algorithms for credit scoring An update of research.pdf:pdf},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Credit scoring,Data mining,Forecasting benchmark,OR in banking},
month = {nov},
number = {1},
pages = {124--136},
publisher = {Elsevier},
title = {{Benchmarking state-of-the-art classification algorithms for credit scoring: An update of research}},
volume = {247},
year = {2015}
}


@article{Crone2012,
abstract = {To date, best practice in sampling credit applicants has been established based largely on expert opinion, which generally recommends that small samples of 1500 instances each of both goods and bads are sufficient, and that the heavily biased datasets observed should be balanced by undersampling the majority class. Consequently, the topics of sample sizes and sample balance have not been subject to either formal study in credit scoring, or empirical evaluations across different data conditions and algorithms of varying efficiency. This paper describes an empirical study of instance sampling in predicting consumer repayment behaviour, evaluating the relative accuracies of logistic regression, discriminant analysis, decision trees and neural networks on two datasets across 20 samples of increasing size and 29 rebalanced sample distributions created by gradually under- and over-sampling the goods and bads respectively. The paper makes a practical contribution to model building on credit scoring datasets, and provides evidence that using samples larger than those recommended in credit scoring practice provides a significant increase in accuracy across algorithms. {\textcopyright} 2011.},
annote = {- 1500 instances of goods and bads would be enoughfor robust predictions
- Popular methods: Logistsic/Discrimant Analysis/RegressionTrees
- Under and oversampling techniques. Maybe important when considering effieincy
-Variables like income, better if catagorized by income.3 to 6 dummies would be sufficient [future work]
-CART cdon't perform so good with categories},
author = {Crone, Sven F. and Finlay, Steven},
doi = {10.1016/j.ijforecast.2011.07.006},
file = {:C$\backslash$:/Users/diego/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crone, Finlay - 2012 - Instance sampling in credit scoring An empirical study of sample size and balancing.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Balancing,Credit scoring,Data pre-processing,Over-sampling,Sample size,Under-sampling},
month = {jan},
number = {1},
pages = {224--238},
title = {{Instance sampling in credit scoring: An empirical study of sample size and balancing}},
volume = {28},
year = {2012}
}
@article{Kruppa2013,
abstract = {Consumer credit scoring is often considered a classification task where clients receive either a good or a bad credit status. Default probabilities provide more detailed information about the creditworthiness of consumers, and they are usually estimated by logistic regression. Here, we present a general framework for estimating individual consumer credit risks by use of machine learning methods. Since a probability is an expected value, all nonparametric regression approaches which are consistent for the mean are consistent for the probability estimation problem. Among others, random forests (RF), k-nearest neighbors (kNN), and bagged k-nearest neighbors (bNN) belong to this class of consistent nonparametric regression approaches. We apply the machine learning methods and an optimized logistic regression to a large dataset of complete payment histories of short-termed installment credits. We demonstrate probability estimation in Random Jungle, an RF package written in C++ with a generalized framework for fast tree growing, probability estimation, and classification. We also describe an algorithm for tuning the terminal node size for probability estimation. We demonstrate that regression RF outperforms the optimized logistic regression model, kNN, and bNN on the test data of the short-term installment credits. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Kruppa, Jochen and Schwarz, Alexandra and Arminger, Gerhard and Ziegler, Andreas},
doi = {10.1016/j.eswa.2013.03.019},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Credit scoring,Logistic regression,Machine learning,Probability estimation,Probability machines,Random forest},
title = {{Consumer credit risk: Individual probability estimates using machine learning}},
year = {2013}
}
@article{Stelzer2019,
abstract = {This study conducts a benchmarking study, comparing 23 different statistical and machine learning methods in a credit scoring application. In order to do so, the models' performance is evaluated over four different data sets in combination with five data sampling strategies to tackle existing class imbalances in the data. Six different performance measures are used to cover different aspects of predictive performance. The results indicate a strong superiority of ensemble methods and show that simple sampling strategies deliver better results than more sophisticated ones.},
archivePrefix = {arXiv},
arxivId = {1907.12996},
author = {Stelzer, Anna},
eprint = {1907.12996},
file = {:C$\backslash$:/Users/diego/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stelzer - 2019 - Predicting credit default probabilities using machine learning techniques in the face of unequal class distributions.pdf:pdf},
month = {jul},
title = {{Predicting credit default probabilities using machine learning techniques in the face of unequal class distributions}},
url = {http://arxiv.org/abs/1907.12996},
year = {2019}
}

@article{Baesens,
author = {Baesens, Bart and Van Gestel, Tony and Viaene, Stijn and STEPANOVA, M. and Suykens, Johan and Vanthienen, Jan},
year = {2003},
month = {06},
pages = {},
title = {Benchmarking state-of-the-art classification algorithms for credit scoring},
volume = {54},
journal = {Journal of the Operational Research Society},
doi = {10.1057/palgrave.jors.2601545}
}

@book{Geron2019,
Abstract = {Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how.By using concrete examples, minimal theory, and two production-ready Python frameworks—Scikit-Learn and TensorFlow—author Aurélien Géron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started.Explore the machine learning landscape, particularly neural netsUse Scikit-Learn to track an example machine-learning project end-to-endExplore several training models, including support vector machines, decision trees, rando},
Author = {Aurélien, Géron},
ISBN = {9781492032649},
Publisher = {O'Reilly Media},
Title = {Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow : Concepts, Tools, and Techniques to Build Intelligent Systems.},
Volume = {Second edition},
Year = {2019},
}

@Book{Bishop2006,
  author =       "Bishop, Christopher M.",
  title =        "Pattern Recognition and Machine Learning",
  publisher =    "Springer",
  year =         "2006",
  ISBN =         "978-0387-31073-2",
  url = "http://research.microsoft.com/en-us/um/people/cmbishop/prml/",
  bib2html_rescat = "General ML",
}

@article{Yeh2009,
abstract = {This research aimed at the case of customers' default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel "Sorting Smoothing Method" to estimate the real probability of default. With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X), the simple linear regression result (Y = A + BX) shows that the forecasting model produced by artificial neural network has the highest coefficient of determination; its regression intercept (A) is close to zero, and regression coefficient (B) to one. Therefore, among the six data mining techniques, artificial neural network is the only one that can accurately estimate the real probability of default. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Yeh, I. Cheng and hui Lien, Che},
doi = {10.1016/j.eswa.2007.12.020},
file = {:C\:/Users/carol/Desktop/DefaultCreditCardClients_yeh_2009 (1).pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Banking,Data mining,Neural network,Probability},
number = {2 PART 1},
pages = {2473--2480},
publisher = {Elsevier Ltd},
title = {{The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients}},
url = {http://dx.doi.org/10.1016/j.eswa.2007.12.020},
volume = {36},
year = {2009}
}

@article{Liang2019,
abstract = {Evaluating and predicting the repayment ability of the loaners is important for the banks to minimize the risk of loan payment default. By this reason, there is a system created by the banks to process the loan request based on the loaners' status, such as employment status, credit history, etc.. However, the current existing evaluation system might not be appropriate to evaluate some loaners repayment ability, such as students or people without credit histories. In order to properly assess the repayment ability of all groups of people, we trained various machine learning models on a Kaggle dataset, Home Credit Default Risk, and evaluated the importance of all the features used. Then, based on the importance score of the features, we analyze and select the most identifiable features to predict the repayment ability of the loaner.},
author = {Liang, Yiyun and Jin, Xiaomeng and Wang, Zihan},
file = {:C\:/Users/carol/Desktop/Spring 2021/ML/Project/Bibliography/26644913.pdf:pdf},
mendeley-groups = {ML},
title = {{Loanliness: Predicting Loan Repayment Ability by Using Machine Learning Methods}},
url = {https://github.com/Yiyun-Liang/loanliness},
year = {2019}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{Han_2019,
author = {Han,Chang},
title = {Loan Repayment Prediction Using Machine Learning Algorithms},
Url = {https://escholarship.org/uc/item/9cc4t85b#main2},
journal = {UCLA Electronic Theses and Dissertations},
year = {2019},
}

@inproceedings{soton403913,
       booktitle = {Positioning and Power in Academic Publishing: Players, Agents and Agendas},
          editor = {Fernando Loizides and Birgit Scmidt},
           title = {Jupyter Notebooks ? a publishing format for reproducible computational workflows},
          author = {Thomas Kluyver and Benjamin Ragan-Kelley and Fernando P{\'e}rez and Brian Granger and Matthias Bussonnier and Jonathan Frederic and Kyle Kelley and Jessica Hamrick and Jason Grout and Sylvain Corlay and Paul Ivanov and Dami{\'a}n Avila and Safia Abdalla and Carol Willing and  Jupyter development team},
       publisher = {IOS Press},
            year = {2016},
           pages = {87--90},
             url = {https://eprints.soton.ac.uk/403913/},
        abstract = {It is increasingly necessary for researchers in all fields to write computer code, and in order to reproduce research results, it is important that this code is published. We present Jupyter notebooks, a document format for publishing code, results and explanations in a form that is both readable and executable. We discuss various tools and use cases for notebook documents.}
}
